{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from util import full_layer, config, mse\n",
    "\n",
    "class IModel(object):\n",
    "    def train(self, x):\n",
    "        pass\n",
    "    def predict(self, x):\n",
    "        pass\n",
    "\n",
    "\n",
    "class SingleHiddenNN(IModel):\n",
    "    def __init__(self, input_shape=[128, 9216], n_hidden=100, n_y=30, obj_fcn=mse):\n",
    "        '''\n",
    "        input_shape: (batch_size, input dimension); tuple\n",
    "        n_hidden: # latent variables; int\n",
    "        n_y: # output features; int\n",
    "        '''\n",
    "        batch_size, n_x = input_shape\n",
    "        x = tf.placeholder(shape=(None, n_x), dtype=tf.float32, name='x')\n",
    "        y = tf.placeholder(shape=(None, n_y), dtype=tf.float32, name='y')\n",
    "        lr = tf.placeholder(dtype=tf.float32)\n",
    "        \n",
    "        h1 = full_layer(x, n_hidden, 'Hidden01')\n",
    "        yhat = full_layer(h1, n_y, 'Output', nonlinear=tf.identity)\n",
    "\n",
    "        # 這邊沒啥重要的，只是叫這個物件把某些 attributes 存下來而已\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.yhat = yhat\n",
    "\n",
    "        # Training\n",
    "        self.objective = obj_fcn(y, yhat)\n",
    "        self.optimizer = tf.train.AdamOptimizer(self.lr).minimize(self.objective)\n",
    "\n",
    "        self.sess = tf.Session(config=config)\n",
    "        \n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.sess.run(self.yhat, feed_dict={self.x: x})\n",
    "\n",
    "    def train(self, train_batches, valid_set=None, lr=1e-2, n_epoch=100):        \n",
    "        # 叫 graph 把 initialization 排進行程裡\n",
    "        init = tf.initialize_all_variables() \n",
    "        self.sess.run(init)  # 做了這步之後，所有的 weight 才會有值\n",
    "\n",
    "        # 把 computational graph 畫出來\n",
    "        summary_writer = tf.train.SummaryWriter(\n",
    "            logdir='model1',\n",
    "            graph=self.sess.graph)\n",
    "\n",
    "        loss_train_record = list() # np.zeros(n_epoch)\n",
    "        loss_valid_record = list() # np.zeros(n_epoch)\n",
    "        start_time = time.gmtime()\n",
    "\n",
    "        n_batch = train_batches.n_batch\n",
    "        for i in range(n_epoch):\n",
    "            loss_train_sum = 0.0\n",
    "            loss_valid_sum = 0.0\n",
    "            for x, y in train_batches:\n",
    "                # ==== Training ====\n",
    "                _, loss_train = self.sess.run(\n",
    "                    [self.optimizer, self.objective], \n",
    "                    feed_dict={\n",
    "                        self.x: x, \n",
    "                        self.y: y,\n",
    "                        self.lr: lr})\n",
    "                # ==== Evaluation ====\n",
    "                loss_valid = self.sess.run(\n",
    "                    self.objective, \n",
    "                    feed_dict={\n",
    "                        self.x: valid_set.images, \n",
    "                        self.y: valid_set.labels})\n",
    "                loss_train_sum += loss_train\n",
    "                loss_valid_sum += loss_valid\n",
    "            # Warning: 這不是 Tensorflow 的 style\n",
    "            print 'Epoch %04d, %.8f, %.8f,  %0.8f' % (\n",
    "                i, loss_train_sum/n_batch, loss_valid_sum/n_batch,\n",
    "                loss_train_sum/loss_valid_sum)\n",
    "            loss_train_record.append(loss_train_sum)    # np.log10()\n",
    "            loss_valid_record.append(loss_valid_sum)    # np.log10()\n",
    "\n",
    "        end_time = time.gmtime()\n",
    "        print time.strftime('%H:%M:%S', start_time)\n",
    "        print time.strftime('%H:%M:%S', end_time)\n",
    "        self._error_plot(loss_train_record, loss_valid_record)\n",
    "\n",
    "    def _error_plot(self, trn_loss, vld_loss):\n",
    "        '''\n",
    "        trn_loss: training_loss_record\n",
    "        vld_loss: validation_loss_record\n",
    "        '''\n",
    "        plt.figure()\n",
    "        plt.plot(trn_loss, label='train')\n",
    "        plt.plot(vld_loss, c='r', label='validation')\n",
    "        plt.xlabel('mini-batch')\n",
    "        plt.ylabel('loss')\n",
    "        plt.yscale('log')\n",
    "        # plt.ylim(1e-3, 1e-2)\n",
    "        plt.legend()\n",
    "        plt.savefig('imgs/model1-(fully-connected)-loss.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Placeholder: 一種餵 data 進去 computaional graph 的方式\n",
    "讓我們從 constructor (`__init__`) 開始講  \n",
    "\n",
    "```python\n",
    "x = tf.placeholder(shape=(None, n_x), dtype=tf.float32, name='x')\n",
    "```\n",
    "這裡有個新東西叫 `tf.placeholder`  \n",
    "他是一個很方便的東西：\n",
    "1. 它讓你在建立 computational graph 的時候，可以先用 placeholder 指定好 input 的形狀，形成一個沒有內容的 tensor，以便讓 TF 能夠處理。\n",
    "2. tf.placeholder 物件可以有未知大小的維度，用 None 代表\n",
    "\n",
    "另一個要提點的地方是：learning rate (lr) 也是用 placeholder 來存  \n",
    "如此一來， learning rate 就不用在 constructor 內被訂死。  \n",
    "請注意這點。  \n",
    "(用 tf.Variable 存很沒意思。因為它歸根究柢並非 model parameter)\n",
    "\n",
    "\n",
    "**Hint**:\n",
    "1. tf.placeholder 一定要指定 data type。在 GPU 運算裡，通常使用的是 tf.float32 (請不要用 tf.float64)\n",
    "2. 第一軸通常用來代表 batch size；我們會把它填成 None 其實只是為了方便。\n",
    "3. 如果有使用 tf.placeholder，那麼之後在 training 的時候，就要把真正的資料餵進去(feed)。\n",
    "4. 實際上 TF 有提供其他餵資料的方法。但很複雜，我在此 tutorial 中不會講到。(有興趣的可以去看 TF 的 CNN cifar10 教學\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 指定Objective 以及 Optimizer\n",
    "我這邊為了維持 objective 的可變性，我是在 class 外面定義 objective，然後透過 constructor 傳進來的。我這邊為了符合 Nouri 的作法，將 mean square error 定義如下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mse(y, yhat, name='objective_mse'):\n",
    "    with tf.variable_scope(name):\n",
    "        obj = tf.sub(y, yhat)   # sub: 矩陣減法\n",
    "        obj = tf.square(obj)\n",
    "        obj = tf.reduce_mean(obj) # 取整個 tensor 的平均\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定義好 MSE之後，接下來要怎麼 minimize 它了。  \n",
    "在沒有 Theano/Tensorflow 的時代，唯一的方法就是親手去導 gradient  \n",
    "但現在你唯一要做的，就是呼叫 optimizer 而已。  \n",
    "Optimizer 在做的事情無非就是計算 gradient 以及更新參數。  \n",
    "都是些苦工。而現在，你有 TF 幫你搞定這些苦功了！\n",
    "\n",
    "\n",
    "TF 目前提供了 7 種 optimizer，從最陽春的 SGD 到 Adam 都有。\n",
    "我們這邊就無腦用 Adam 囉。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session 與 Training\n",
    "之前講過，TF把流程分成 symbolic 和 numerical 兩部分。  \n",
    "當你要真正把資料餵進去的時候（也就是要 training）的時候，一切才真正進入 numerical 階段。\n",
    "\n",
    "在開始這個階段之前，你需要 `tf.Session`物件；他負責啟動/運行我們定義好的 computational graph  \n",
    "它和 training 緊密相關，所以我們直接看 `train()`  \n",
    "\n",
    "### Initialize all Variables\n",
    "首先又是一個 TF 特有的東西：  \n",
    "```python\n",
    "init = tf.initialize_all_variables()\n",
    "self.sess.run(init)\n",
    "```\n",
    "之前講過：在第一個階段，所有的運作都是 symbolic。  \n",
    "所以，即使**宣告**了一個變數，並且給了初始值。  \n",
    "該變數也不會真的有值──直到**初始化**為止。  \n",
    "而 TF 中，變數需要透過上面兩行來初始化。  \n",
    "有點類似在 C++ 裡面，declaration 和 initialization 是分開的一樣。  \n",
    "只是 TF 把 initialization 全部集中在一個函式裡面而已。\n",
    "然後請記得叫 session 去 run 它。\n",
    "不然所有的 parameters 仍然沒有真正被執行。(\n",
    "\n",
    "### Session\n",
    "(然後請先略過 summary_writer 那段)\n",
    "我們直接進入 training 的部分。\n",
    "\n",
    "```python\n",
    "_, loss_train = self.sess.run(\n",
    "    [self.optimizer, self.objective], \n",
    "    feed_dict={self.x: x, self.y: y, self.lr: lr})\n",
    "```\n",
    "\n",
    "`sess` 是 `tf.Session` 物件，其 `run` method 負責把 symoblic 和 numeric 兩個世界連接起來。  \n",
    "這邊必須說明一下：  \n",
    "Computational graph 就像一局擺好的骨牌，data (本段 code 中的 x 和 y) 就像是那第一張骨牌。  \n",
    "所以第一張骨牌推下去之後，所有之前定義過的東西都可以取值了。\n",
    "這邊我們要求取 `self.optimizer` 和 `self.objective` 的值。\n",
    "objective 是為了讓我們了解現在 training 的情況如何 (我們想知道它有沒有在降)  \n",
    "那麼 optimizer 呢? 為何要取其值? 其值代表什麼意義?  \n",
    "其實它不會回傳有用的值。  \n",
    "前面講過，optimizer 做兩件事：  \n",
    "1. 算 gradient\n",
    "2. 更新參數\n",
    "\n",
    "所以這邊叫 Session 去 run 它，意思就是要做這兩件事。\n",
    "\n",
    "\n",
    "### Feed Your Networks\n",
    "另一個要說明的是 `feed_dict`  \n",
    "`run` 要求我們把需要傳進去的資料都存在 `dict` 裡面（因為變數可能多、可能少，用 `dict` 最方便）。  \n",
    "我這邊是用一個臨時建成的 dict 來放。\n",
    "\n",
    "Hint:\n",
    "1. 事實上，training 永遠是 ad-hoc。所以其實未必應該寫在 class 裡面。我這邊的寫法僅供參考。\n",
    "2. 我這裡假設傳進來的 x 是 iterator，每 iterate 一次都會產生新的 batch。  \n",
    "雖然未必要這樣寫，但把資料變成 iterator 會很方便。詳請可以參考 util.py。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 關於 evaluation\n",
    "在 training loop 中，我還放了 evaluation\n",
    "說真的，這個寫法並不好。  \n",
    "因為通常我們不會每個 mini-batch 都跑 eval  \n",
    "而且在資料龐大的時候，這種方法更顯荒謬。  \n",
    "在 TF 的 Cifar10 的官方範例中，作者是用另外一支 script 去跑 eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0000, 226.53145993, 218.66834408,  1.03595910\n",
      "Epoch 0001, 0.16646391, 0.16469980,  1.01071107\n",
      "Epoch 0002, 0.16365379, 0.16138339,  1.01406836\n",
      "Epoch 0003, 0.15735120, 0.15493305,  1.01560769\n",
      "Epoch 0004, 0.14952740, 0.14724015,  1.01553412\n",
      "Epoch 0005, 0.14133042, 0.13906431,  1.01629541\n",
      "Epoch 0006, 0.13299465, 0.13077066,  1.01700676\n",
      "Epoch 0007, 0.12481390, 0.12255740,  1.01841179\n",
      "Epoch 0008, 0.11672090, 0.11456048,  1.01885836\n",
      "Epoch 0009, 0.10882099, 0.10685418,  1.01840647\n",
      "Epoch 0010, 0.10153027, 0.09950666,  1.02033648\n",
      "Epoch 0011, 0.09453835, 0.09253419,  1.02165851\n",
      "Epoch 0012, 0.08796980, 0.08596051,  1.02337451\n",
      "Epoch 0013, 0.08154885, 0.07979735,  1.02194940\n",
      "Epoch 0014, 0.07585405, 0.07404049,  1.02449418\n",
      "Epoch 0015, 0.07035331, 0.06868178,  1.02433741\n",
      "Epoch 0016, 0.06556433, 0.06369928,  1.02927893\n",
      "Epoch 0017, 0.06062266, 0.05907342,  1.02622563\n",
      "Epoch 0018, 0.05631913, 0.05479542,  1.02780713\n",
      "Epoch 0019, 0.05226400, 0.05084553,  1.02789755\n",
      "Epoch 0020, 0.04860183, 0.04719448,  1.02982026\n",
      "Epoch 0021, 0.04512022, 0.04382235,  1.02961658\n",
      "Epoch 0022, 0.04202285, 0.04071152,  1.03221041\n",
      "Epoch 0023, 0.03901482, 0.03784512,  1.03090757\n",
      "Epoch 0024, 0.03636494, 0.03520103,  1.03306450\n",
      "Epoch 0025, 0.03387118, 0.03276496,  1.03376236\n",
      "Epoch 0026, 0.03157922, 0.03051487,  1.03487964\n",
      "Epoch 0027, 0.02947651, 0.02844142,  1.03639400\n",
      "Epoch 0028, 0.02748833, 0.02653088,  1.03608827\n",
      "Epoch 0029, 0.02566509, 0.02476982,  1.03614336\n",
      "Epoch 0030, 0.02403882, 0.02314190,  1.03875778\n",
      "Epoch 0031, 0.02244811, 0.02164126,  1.03728250\n",
      "Epoch 0032, 0.02106684, 0.02025626,  1.04001623\n",
      "Epoch 0033, 0.01982512, 0.01897834,  1.04461851\n",
      "Epoch 0034, 0.01856821, 0.01779803,  1.04327348\n",
      "Epoch 0035, 0.01745463, 0.01670784,  1.04469696\n",
      "Epoch 0036, 0.01639597, 0.01570183,  1.04420744\n",
      "Epoch 0037, 0.01544915, 0.01477520,  1.04561307\n",
      "Epoch 0038, 0.01457774, 0.01391761,  1.04743079\n",
      "Epoch 0039, 0.01373960, 0.01312846,  1.04655039\n",
      "Epoch 0040, 0.01300732, 0.01239982,  1.04899259\n",
      "Epoch 0041, 0.01233512, 0.01172669,  1.05188445\n",
      "Epoch 0042, 0.01172236, 0.01110510,  1.05558338\n",
      "Epoch 0043, 0.01107210, 0.01053275,  1.05120692\n",
      "Epoch 0044, 0.01054724, 0.01000539,  1.05415564\n",
      "Epoch 0045, 0.01005588, 0.00952055,  1.05622956\n",
      "Epoch 0046, 0.00957663, 0.00907187,  1.05564014\n",
      "Epoch 0047, 0.00912674, 0.00865950,  1.05395597\n",
      "Epoch 0048, 0.00875598, 0.00828019,  1.05746051\n",
      "Epoch 0049, 0.00837946, 0.00793024,  1.05664652\n",
      "Epoch 0050, 0.00808504, 0.00760807,  1.06269269\n",
      "Epoch 0051, 0.00775125, 0.00731290,  1.05994180\n",
      "Epoch 0052, 0.00744252, 0.00704075,  1.05706340\n",
      "Epoch 0053, 0.00722447, 0.00679156,  1.06374277\n",
      "Epoch 0054, 0.00697998, 0.00656268,  1.06358763\n",
      "Epoch 0055, 0.00675312, 0.00635187,  1.06316992\n",
      "Epoch 0056, 0.00652192, 0.00615899,  1.05892773\n",
      "Epoch 0057, 0.00637835, 0.00598324,  1.06603483\n",
      "Epoch 0058, 0.00620323, 0.00582204,  1.06547324\n",
      "Epoch 0059, 0.00601513, 0.00567480,  1.05997311\n",
      "Epoch 0060, 0.00589131, 0.00553997,  1.06341835\n",
      "Epoch 0061, 0.00573991, 0.00541688,  1.05963502\n",
      "Epoch 0062, 0.00558368, 0.00530417,  1.05269625\n",
      "Epoch 0063, 0.00553082, 0.00520089,  1.06343730\n",
      "Epoch 0064, 0.00542600, 0.00510657,  1.06255302\n",
      "Epoch 0065, 0.00533971, 0.00502110,  1.06345412\n",
      "Epoch 0066, 0.00526715, 0.00494338,  1.06549647\n",
      "Epoch 0067, 0.00518236, 0.00487260,  1.06357366\n",
      "Epoch 0068, 0.00512122, 0.00480783,  1.06518343\n",
      "Epoch 0069, 0.00503699, 0.00474928,  1.06058013\n",
      "Epoch 0070, 0.00498062, 0.00469636,  1.06052927\n",
      "Epoch 0071, 0.00492968, 0.00464816,  1.06056624\n",
      "Epoch 0072, 0.00486261, 0.00460472,  1.05600676\n",
      "Epoch 0073, 0.00484020, 0.00456515,  1.06025027\n",
      "Epoch 0074, 0.00482573, 0.00452956,  1.06538736\n",
      "Epoch 0075, 0.00475352, 0.00449732,  1.05696667\n",
      "Epoch 0076, 0.00470227, 0.00446795,  1.05244344\n",
      "Epoch 0077, 0.00469571, 0.00444175,  1.05717410\n",
      "Epoch 0078, 0.00466347, 0.00441822,  1.05550945\n",
      "Epoch 0079, 0.00466829, 0.00439691,  1.06171977\n",
      "Epoch 0080, 0.00463855, 0.00437772,  1.05958223\n",
      "Epoch 0081, 0.00464095, 0.00436062,  1.06428544\n",
      "Epoch 0082, 0.00457473, 0.00434539,  1.05277763\n",
      "Epoch 0083, 0.00458124, 0.00433141,  1.05767878\n",
      "Epoch 0084, 0.00454137, 0.00431902,  1.05148192\n",
      "Epoch 0085, 0.00454741, 0.00430804,  1.05556406\n",
      "Epoch 0086, 0.00453716, 0.00429816,  1.05560464\n",
      "Epoch 0087, 0.00451335, 0.00428940,  1.05220868\n",
      "Epoch 0088, 0.00450560, 0.00428160,  1.05231718\n",
      "Epoch 0089, 0.00455362, 0.00427477,  1.06523324\n",
      "Epoch 0090, 0.00454044, 0.00426864,  1.06367544\n",
      "Epoch 0091, 0.00449415, 0.00426315,  1.05418560\n",
      "Epoch 0092, 0.00450719, 0.00425832,  1.05844441\n",
      "Epoch 0093, 0.00449848, 0.00425411,  1.05744225\n",
      "Epoch 0094, 0.00449595, 0.00425055,  1.05773502\n",
      "Epoch 0095, 0.00447181, 0.00424738,  1.05283993\n",
      "Epoch 0096, 0.00447408, 0.00424447,  1.05409741\n",
      "Epoch 0097, 0.00446333, 0.00424188,  1.05220496\n",
      "Epoch 0098, 0.00446382, 0.00423966,  1.05287281\n",
      "Epoch 0099, 0.00446827, 0.00423767,  1.05441485\n",
      "Epoch 0100, 0.00448141, 0.00423610,  1.05790811\n",
      "Epoch 0101, 0.00446429, 0.00423473,  1.05420837\n",
      "Epoch 0102, 0.00445669, 0.00423354,  1.05270850\n",
      "Epoch 0103, 0.00445892, 0.00423251,  1.05349328\n",
      "Epoch 0104, 0.00447165, 0.00423162,  1.05672310\n",
      "Epoch 0105, 0.00444474, 0.00423073,  1.05058564\n",
      "Epoch 0106, 0.00443681, 0.00423013,  1.04885819\n",
      "Epoch 0107, 0.00444143, 0.00422975,  1.05004559\n",
      "Epoch 0108, 0.00448541, 0.00422950,  1.06050739\n",
      "Epoch 0109, 0.00446119, 0.00422917,  1.05486210\n",
      "Epoch 0110, 0.00443640, 0.00422893,  1.04905884\n",
      "Epoch 0111, 0.00447271, 0.00422858,  1.05773416\n",
      "Epoch 0112, 0.00445826, 0.00422834,  1.05437583\n",
      "Epoch 0113, 0.00446008, 0.00422824,  1.05482992\n",
      "Epoch 0114, 0.00442498, 0.00422804,  1.04657931\n",
      "Epoch 0115, 0.00446200, 0.00422795,  1.05535599\n",
      "Epoch 0116, 0.00445649, 0.00422802,  1.05403773\n",
      "Epoch 0117, 0.00444264, 0.00422802,  1.05075975\n",
      "Epoch 0118, 0.00444934, 0.00422803,  1.05234403\n",
      "Epoch 0119, 0.00446590, 0.00422785,  1.05630570\n",
      "Epoch 0120, 0.00442803, 0.00422778,  1.04736472\n",
      "Epoch 0121, 0.00442722, 0.00422793,  1.04713447\n",
      "Epoch 0122, 0.00442544, 0.00422791,  1.04672182\n",
      "Epoch 0123, 0.00445690, 0.00422798,  1.05414514\n",
      "Epoch 0124, 0.00446326, 0.00422815,  1.05560732\n",
      "Epoch 0125, 0.00443336, 0.00422822,  1.04851589\n",
      "Epoch 0126, 0.00444188, 0.00422817,  1.05054273\n",
      "Epoch 0127, 0.00445516, 0.00422804,  1.05371741\n",
      "Epoch 0128, 0.00445483, 0.00422803,  1.05364219\n",
      "Epoch 0129, 0.00440419, 0.00422834,  1.04158897\n",
      "Epoch 0130, 0.00445998, 0.00422840,  1.05476751\n",
      "Epoch 0131, 0.00443897, 0.00422825,  1.04983585\n",
      "Epoch 0132, 0.00444210, 0.00422827,  1.05057232\n",
      "Epoch 0133, 0.00444081, 0.00422849,  1.05021174\n",
      "Epoch 0134, 0.00446650, 0.00422858,  1.05626353\n",
      "Epoch 0135, 0.00443650, 0.00422870,  1.04914236\n",
      "Epoch 0136, 0.00446095, 0.00422876,  1.05490676\n",
      "Epoch 0137, 0.00443101, 0.00422865,  1.04785465\n",
      "Epoch 0138, 0.00444610, 0.00422847,  1.05146817\n",
      "Epoch 0139, 0.00445644, 0.00422860,  1.05388281\n",
      "Epoch 0140, 0.00445028, 0.00422871,  1.05239790\n",
      "Epoch 0141, 0.00444460, 0.00422877,  1.05103909\n",
      "Epoch 0142, 0.00444973, 0.00422877,  1.05225118\n",
      "Epoch 0143, 0.00445608, 0.00422886,  1.05373038\n",
      "Epoch 0144, 0.00444814, 0.00422867,  1.05189919\n",
      "Epoch 0145, 0.00442599, 0.00422850,  1.04670581\n",
      "Epoch 0146, 0.00445832, 0.00422849,  1.05435269\n",
      "Epoch 0147, 0.00445158, 0.00422879,  1.05268312\n",
      "Epoch 0148, 0.00448367, 0.00422882,  1.06026513\n",
      "Epoch 0149, 0.00444998, 0.00422887,  1.05228550\n",
      "Epoch 0150, 0.00446377, 0.00422885,  1.05554960\n",
      "Epoch 0151, 0.00443822, 0.00422873,  1.04953848\n",
      "Epoch 0152, 0.00444144, 0.00422895,  1.05024768\n",
      "Epoch 0153, 0.00446534, 0.00422893,  1.05590275\n",
      "Epoch 0154, 0.00446819, 0.00422891,  1.05658380\n",
      "Epoch 0155, 0.00442422, 0.00422899,  1.04616515\n",
      "Epoch 0156, 0.00446143, 0.00422916,  1.05492263\n",
      "Epoch 0157, 0.00446026, 0.00422924,  1.05462549\n",
      "Epoch 0158, 0.00444151, 0.00422922,  1.05019582\n",
      "Epoch 0159, 0.00443451, 0.00422901,  1.04859219\n",
      "Epoch 0160, 0.00443060, 0.00422925,  1.04760816\n",
      "Epoch 0161, 0.00447107, 0.00422933,  1.05716004\n",
      "Epoch 0162, 0.00442906, 0.00422928,  1.04723735\n",
      "Epoch 0163, 0.00445555, 0.00422924,  1.05351100\n",
      "Epoch 0164, 0.00446267, 0.00422915,  1.05521660\n",
      "Epoch 0165, 0.00446430, 0.00422876,  1.05569934\n",
      "Epoch 0166, 0.00443458, 0.00422860,  1.04871106\n",
      "Epoch 0167, 0.00442597, 0.00422889,  1.04660276\n",
      "Epoch 0168, 0.00445298, 0.00422908,  1.05294171\n",
      "Epoch 0169, 0.00443951, 0.00422893,  1.04979615\n",
      "Epoch 0170, 0.00445460, 0.00422899,  1.05334783\n",
      "Epoch 0171, 0.00442259, 0.00422902,  1.04577366\n",
      "Epoch 0172, 0.00444165, 0.00422911,  1.05025508\n",
      "Epoch 0173, 0.00444091, 0.00422911,  1.05008010\n",
      "Epoch 0174, 0.00446687, 0.00422940,  1.05614698\n",
      "Epoch 0175, 0.00450117, 0.00422899,  1.06435987\n",
      "Epoch 0176, 0.00443589, 0.00422905,  1.04891031\n",
      "Epoch 0177, 0.00445619, 0.00422908,  1.05370319\n",
      "Epoch 0178, 0.00444708, 0.00422901,  1.05156565\n",
      "Epoch 0179, 0.00446120, 0.00422917,  1.05486580\n",
      "Epoch 0180, 0.00446340, 0.00422911,  1.05539947\n",
      "Epoch 0181, 0.00443391, 0.00422915,  1.04841595\n",
      "Epoch 0182, 0.00445149, 0.00422915,  1.05257278\n",
      "Epoch 0183, 0.00447536, 0.00422918,  1.05821013\n",
      "Epoch 0184, 0.00444556, 0.00422912,  1.05117773\n",
      "Epoch 0185, 0.00445165, 0.00422893,  1.05266695\n",
      "Epoch 0186, 0.00444930, 0.00422871,  1.05216546\n",
      "Epoch 0187, 0.00445911, 0.00422877,  1.05447036\n",
      "Epoch 0188, 0.00442904, 0.00422869,  1.04738010\n",
      "Epoch 0189, 0.00447951, 0.00422871,  1.05931012\n",
      "Epoch 0190, 0.00444439, 0.00422876,  1.05099201\n",
      "Epoch 0191, 0.00443552, 0.00422885,  1.04887302\n",
      "Epoch 0192, 0.00444319, 0.00422888,  1.05067812\n",
      "Epoch 0193, 0.00443075, 0.00422891,  1.04772877\n",
      "Epoch 0194, 0.00442232, 0.00422888,  1.04574258\n",
      "Epoch 0195, 0.00447898, 0.00422888,  1.05914106\n",
      "Epoch 0196, 0.00445324, 0.00422873,  1.05309054\n",
      "Epoch 0197, 0.00444129, 0.00422883,  1.05024237\n",
      "Epoch 0198, 0.00446739, 0.00422894,  1.05638449\n",
      "Epoch 0199, 0.00444799, 0.00422899,  1.05178439\n",
      "Epoch 0200, 0.00447109, 0.00422871,  1.05731824\n",
      "Epoch 0201, 0.00443327, 0.00422855,  1.04841156\n",
      "Epoch 0202, 0.00444032, 0.00422881,  1.05001784\n",
      "Epoch 0203, 0.00443665, 0.00422915,  1.04906509\n",
      "Epoch 0204, 0.00442231, 0.00422886,  1.04574679\n",
      "Epoch 0205, 0.00445187, 0.00422877,  1.05275781\n",
      "Epoch 0206, 0.00442610, 0.00422887,  1.04663917\n",
      "Epoch 0207, 0.00443243, 0.00422897,  1.04810923\n",
      "Epoch 0208, 0.00447560, 0.00422885,  1.05835016\n",
      "Epoch 0209, 0.00445649, 0.00422898,  1.05379577\n",
      "Epoch 0210, 0.00442483, 0.00422923,  1.04624892\n",
      "Epoch 0211, 0.00446983, 0.00422907,  1.05692809\n",
      "Epoch 0212, 0.00446405, 0.00422900,  1.05558155\n",
      "Epoch 0213, 0.00442903, 0.00422900,  1.04729953\n",
      "Epoch 0214, 0.00449239, 0.00422876,  1.06234359\n",
      "Epoch 0215, 0.00444039, 0.00422900,  1.04998678\n",
      "Epoch 0216, 0.00441315, 0.00422887,  1.04357658\n",
      "Epoch 0217, 0.00445938, 0.00422917,  1.05443263\n",
      "Epoch 0218, 0.00446792, 0.00422933,  1.05641434\n",
      "Epoch 0219, 0.00446416, 0.00422918,  1.05556203\n",
      "Epoch 0220, 0.00447475, 0.00422942,  1.05800580\n",
      "Epoch 0221, 0.00444337, 0.00422934,  1.05060620\n",
      "Epoch 0222, 0.00444440, 0.00422913,  1.05090128\n",
      "Epoch 0223, 0.00446324, 0.00422930,  1.05531517\n",
      "Epoch 0224, 0.00446315, 0.00422920,  1.05531730\n",
      "Epoch 0225, 0.00446894, 0.00422912,  1.05670650\n",
      "Epoch 0226, 0.00442946, 0.00422927,  1.04733432\n",
      "Epoch 0227, 0.00443164, 0.00422966,  1.04775340\n",
      "Epoch 0228, 0.00442466, 0.00422912,  1.04623670\n",
      "Epoch 0229, 0.00447454, 0.00422912,  1.05803106\n",
      "Epoch 0230, 0.00446523, 0.00422906,  1.05584574\n",
      "Epoch 0231, 0.00443354, 0.00422883,  1.04840739\n",
      "Epoch 0232, 0.00442156, 0.00422923,  1.04547697\n",
      "Epoch 0233, 0.00441797, 0.00422948,  1.04456531\n",
      "Epoch 0234, 0.00446851, 0.00422917,  1.05659179\n",
      "Epoch 0235, 0.00445177, 0.00422896,  1.05268774\n",
      "Epoch 0236, 0.00443000, 0.00422891,  1.04755149\n",
      "Epoch 0237, 0.00448566, 0.00422932,  1.06060941\n",
      "Epoch 0238, 0.00440730, 0.00422959,  1.04201608\n",
      "Epoch 0239, 0.00443961, 0.00422933,  1.04971879\n",
      "Epoch 0240, 0.00443822, 0.00422865,  1.04956039\n",
      "Epoch 0241, 0.00447301, 0.00422845,  1.05783798\n",
      "Epoch 0242, 0.00442345, 0.00422857,  1.04608672\n",
      "Epoch 0243, 0.00444528, 0.00422880,  1.05119178\n",
      "Epoch 0244, 0.00442356, 0.00422895,  1.04601869\n",
      "Epoch 0245, 0.00443843, 0.00422930,  1.04944795\n",
      "Epoch 0246, 0.00446087, 0.00422966,  1.05466363\n",
      "Epoch 0247, 0.00445869, 0.00422954,  1.05417954\n",
      "Epoch 0248, 0.00441164, 0.00422980,  1.04298856\n",
      "Epoch 0249, 0.00444650, 0.00422956,  1.05129100\n",
      "Epoch 0250, 0.00444180, 0.00422932,  1.05024011\n",
      "Epoch 0251, 0.00446573, 0.00422945,  1.05586518\n",
      "Epoch 0252, 0.00447374, 0.00422909,  1.05784988\n",
      "Epoch 0253, 0.00440777, 0.00422879,  1.04232339\n",
      "Epoch 0254, 0.00444542, 0.00422909,  1.05115333\n",
      "Epoch 0255, 0.00442219, 0.00422920,  1.04563195\n",
      "Epoch 0256, 0.00444491, 0.00422894,  1.05107010\n",
      "Epoch 0257, 0.00446642, 0.00422873,  1.05620798\n",
      "Epoch 0258, 0.00445567, 0.00422885,  1.05363485\n",
      "Epoch 0259, 0.00442768, 0.00422906,  1.04696575\n",
      "Epoch 0260, 0.00443401, 0.00422860,  1.04857512\n",
      "Epoch 0261, 0.00446530, 0.00422915,  1.05583919\n",
      "Epoch 0262, 0.00441902, 0.00422901,  1.04493008\n",
      "Epoch 0263, 0.00444549, 0.00422883,  1.05123439\n",
      "Epoch 0264, 0.00440418, 0.00422888,  1.04145131\n",
      "Epoch 0265, 0.00442837, 0.00422938,  1.04704880\n",
      "Epoch 0266, 0.00446166, 0.00422944,  1.05490610\n",
      "Epoch 0267, 0.00443959, 0.00422943,  1.04969151\n",
      "Epoch 0268, 0.00446332, 0.00423003,  1.05514931\n",
      "Epoch 0269, 0.00443335, 0.00422926,  1.04825619\n",
      "Epoch 0270, 0.00445673, 0.00422894,  1.05386324\n",
      "Epoch 0271, 0.00445211, 0.00422913,  1.05272258\n",
      "Epoch 0272, 0.00442137, 0.00422928,  1.04542017\n",
      "Epoch 0273, 0.00447923, 0.00422920,  1.05912081\n",
      "Epoch 0274, 0.00442727, 0.00422918,  1.04683785\n",
      "Epoch 0275, 0.00439875, 0.00422912,  1.04011065\n",
      "Epoch 0276, 0.00446799, 0.00422915,  1.05647494\n",
      "Epoch 0277, 0.00448754, 0.00422864,  1.06122705\n",
      "Epoch 0278, 0.00442889, 0.00422900,  1.04726638\n",
      "Epoch 0279, 0.00446986, 0.00422938,  1.05685943\n",
      "Epoch 0280, 0.00445996, 0.00422979,  1.05441770\n",
      "Epoch 0281, 0.00445185, 0.00422975,  1.05250809\n",
      "Epoch 0282, 0.00441841, 0.00422964,  1.04463134\n",
      "Epoch 0283, 0.00446933, 0.00422901,  1.05682802\n",
      "Epoch 0284, 0.00443694, 0.00422899,  1.04917185\n",
      "Epoch 0285, 0.00444831, 0.00422903,  1.05185145\n",
      "Epoch 0286, 0.00444427, 0.00422854,  1.05101800\n",
      "Epoch 0287, 0.00443217, 0.00422908,  1.04802332\n",
      "Epoch 0288, 0.00442995, 0.00422934,  1.04743199\n",
      "Epoch 0289, 0.00440948, 0.00422883,  1.04271824\n",
      "Epoch 0290, 0.00443556, 0.00422932,  1.04876456\n",
      "Epoch 0291, 0.00443264, 0.00422982,  1.04794941\n",
      "Epoch 0292, 0.00445857, 0.00422968,  1.05411546\n",
      "Epoch 0293, 0.00444424, 0.00422912,  1.05086504\n",
      "Epoch 0294, 0.00441759, 0.00422938,  1.04449966\n",
      "Epoch 0295, 0.00444280, 0.00422905,  1.05054525\n",
      "Epoch 0296, 0.00443527, 0.00422855,  1.04888757\n",
      "Epoch 0297, 0.00447680, 0.00422918,  1.05855087\n",
      "Epoch 0298, 0.00443202, 0.00422913,  1.04797264\n",
      "Epoch 0299, 0.00444305, 0.00422918,  1.05057011\n",
      "Epoch 0300, 0.00443525, 0.00422974,  1.04858736\n",
      "Epoch 0301, 0.00444207, 0.00422910,  1.05035667\n",
      "Epoch 0302, 0.00444809, 0.00422946,  1.05169214\n",
      "Epoch 0303, 0.00446367, 0.00422911,  1.05546465\n",
      "Epoch 0304, 0.00446559, 0.00422947,  1.05582945\n",
      "Epoch 0305, 0.00442367, 0.00422935,  1.04594420\n",
      "Epoch 0306, 0.00447366, 0.00422923,  1.05779456\n",
      "Epoch 0307, 0.00444528, 0.00422915,  1.05110548\n",
      "Epoch 0308, 0.00444446, 0.00422879,  1.05100134\n",
      "Epoch 0309, 0.00443404, 0.00422925,  1.04842326\n",
      "Epoch 0310, 0.00442916, 0.00422953,  1.04719943\n",
      "Epoch 0311, 0.00446543, 0.00422884,  1.05594537\n",
      "Epoch 0312, 0.00445581, 0.00422891,  1.05365514\n",
      "Epoch 0313, 0.00441300, 0.00422968,  1.04334035\n",
      "Epoch 0314, 0.00445139, 0.00422943,  1.05248002\n",
      "Epoch 0315, 0.00442666, 0.00422903,  1.04673082\n",
      "Epoch 0316, 0.00447317, 0.00422916,  1.05769789\n",
      "Epoch 0317, 0.00446316, 0.00422904,  1.05536161\n",
      "Epoch 0318, 0.00444718, 0.00422902,  1.05158549\n",
      "Epoch 0319, 0.00445946, 0.00422860,  1.05459473\n",
      "Epoch 0320, 0.00446878, 0.00422974,  1.05651308\n",
      "Epoch 0321, 0.00447387, 0.00422921,  1.05785004\n",
      "Epoch 0322, 0.00448068, 0.00422931,  1.05943515\n",
      "Epoch 0323, 0.00442664, 0.00422866,  1.04681925\n",
      "Epoch 0324, 0.00446296, 0.00422868,  1.05540256\n",
      "Epoch 0325, 0.00443056, 0.00422970,  1.04748758\n",
      "Epoch 0326, 0.00444601, 0.00422958,  1.05116964\n",
      "Epoch 0327, 0.00445100, 0.00422947,  1.05237675\n",
      "Epoch 0328, 0.00441651, 0.00422870,  1.04441133\n",
      "Epoch 0329, 0.00444854, 0.00422958,  1.05177037\n",
      "Epoch 0330, 0.00444887, 0.00422914,  1.05195807\n",
      "Epoch 0331, 0.00440713, 0.00422855,  1.04223194\n",
      "Epoch 0332, 0.00441210, 0.00422953,  1.04316410\n",
      "Epoch 0333, 0.00448063, 0.00423010,  1.05922568\n",
      "Epoch 0334, 0.00445087, 0.00422913,  1.05243101\n",
      "Epoch 0335, 0.00440979, 0.00423022,  1.04244964\n",
      "Epoch 0336, 0.00443136, 0.00423002,  1.04759666\n",
      "Epoch 0337, 0.00441964, 0.00422921,  1.04502792\n",
      "Epoch 0338, 0.00441687, 0.00422931,  1.04434643\n",
      "Epoch 0339, 0.00444835, 0.00423004,  1.05160889\n",
      "Epoch 0340, 0.00445446, 0.00422985,  1.05310207\n",
      "Epoch 0341, 0.00442633, 0.00422940,  1.04656209\n",
      "Epoch 0342, 0.00444991, 0.00422980,  1.05203882\n",
      "Epoch 0343, 0.00446354, 0.00422879,  1.05551223\n",
      "Epoch 0344, 0.00445679, 0.00422885,  1.05390245\n",
      "Epoch 0345, 0.00444350, 0.00422857,  1.05082767\n",
      "Epoch 0346, 0.00445687, 0.00422920,  1.05383223\n",
      "Epoch 0347, 0.00444885, 0.00422878,  1.05204019\n",
      "Epoch 0348, 0.00444633, 0.00422893,  1.05140794\n",
      "Epoch 0349, 0.00445936, 0.00422929,  1.05440096\n",
      "Epoch 0350, 0.00444390, 0.00423026,  1.05050344\n",
      "Epoch 0351, 0.00443244, 0.00422903,  1.04809933\n",
      "Epoch 0352, 0.00445866, 0.00422821,  1.05450305\n",
      "Epoch 0353, 0.00441553, 0.00422877,  1.04416419\n",
      "Epoch 0354, 0.00446477, 0.00422785,  1.05603770\n",
      "Epoch 0355, 0.00447485, 0.00422901,  1.05813241\n",
      "Epoch 0356, 0.00445431, 0.00422937,  1.05318490\n",
      "Epoch 0357, 0.00441180, 0.00422905,  1.04321268\n",
      "Epoch 0358, 0.00444877, 0.00423037,  1.05162726\n",
      "Epoch 0359, 0.00441008, 0.00422941,  1.04271807\n",
      "Epoch 0360, 0.00443571, 0.00422832,  1.04904763\n",
      "Epoch 0361, 0.00440253, 0.00422939,  1.04093817\n",
      "Epoch 0362, 0.00444094, 0.00422969,  1.04994483\n",
      "Epoch 0363, 0.00443680, 0.00422982,  1.04893312\n",
      "Epoch 0364, 0.00445060, 0.00422957,  1.05225715\n",
      "Epoch 0365, 0.00447066, 0.00422805,  1.05738056\n",
      "Epoch 0366, 0.00444468, 0.00422873,  1.05106838\n",
      "Epoch 0367, 0.00444435, 0.00422925,  1.05085935\n",
      "Epoch 0368, 0.00443663, 0.00422914,  1.04906231\n",
      "Epoch 0369, 0.00444690, 0.00422975,  1.05133803\n",
      "Epoch 0370, 0.00446946, 0.00423014,  1.05657468\n",
      "Epoch 0371, 0.00445809, 0.00422929,  1.05409898\n",
      "Epoch 0372, 0.00445336, 0.00422863,  1.05314617\n",
      "Epoch 0373, 0.00445246, 0.00422966,  1.05267559\n",
      "Epoch 0374, 0.00441514, 0.00423035,  1.04368181\n",
      "Epoch 0375, 0.00444696, 0.00422928,  1.05146808\n",
      "Epoch 0376, 0.00445653, 0.00422936,  1.05371370\n",
      "Epoch 0377, 0.00447860, 0.00422868,  1.05910132\n",
      "Epoch 0378, 0.00444790, 0.00422942,  1.05165763\n",
      "Epoch 0379, 0.00438971, 0.00422941,  1.03790292\n",
      "Epoch 0380, 0.00442637, 0.00422891,  1.04669318\n",
      "Epoch 0381, 0.00446712, 0.00422921,  1.05625409\n",
      "Epoch 0382, 0.00444941, 0.00422996,  1.05187936\n",
      "Epoch 0383, 0.00443901, 0.00423052,  1.04928191\n",
      "Epoch 0384, 0.00446446, 0.00422965,  1.05551586\n",
      "Epoch 0385, 0.00443358, 0.00422930,  1.04830203\n",
      "Epoch 0386, 0.00445263, 0.00422920,  1.05283034\n",
      "Epoch 0387, 0.00440212, 0.00422882,  1.04098016\n",
      "Epoch 0388, 0.00446247, 0.00422826,  1.05539281\n",
      "Epoch 0389, 0.00447336, 0.00422949,  1.05766143\n",
      "Epoch 0390, 0.00443442, 0.00423003,  1.04831791\n",
      "Epoch 0391, 0.00449505, 0.00422939,  1.06281407\n",
      "Epoch 0392, 0.00446033, 0.00422895,  1.05471403\n",
      "Epoch 0393, 0.00448450, 0.00422899,  1.06042020\n",
      "Epoch 0394, 0.00444505, 0.00423039,  1.05074420\n",
      "Epoch 0395, 0.00439851, 0.00422899,  1.04008438\n",
      "Epoch 0396, 0.00446477, 0.00422902,  1.05574482\n",
      "Epoch 0397, 0.00447628, 0.00422976,  1.05828325\n",
      "Epoch 0398, 0.00442057, 0.00423132,  1.04472440\n",
      "Epoch 0399, 0.00444525, 0.00423039,  1.05079118\n",
      "08:16:41\n",
      "08:19:06\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'level' is an invalid keyword argument for this function",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_code\u001b[0;34m(self, code_obj, result)\u001b[0m\n\u001b[1;32m   2875\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_in_exec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2876\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2877\u001b[0;31m             \u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"To exit: use 'exit', 'quit', or Ctrl-D.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2878\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcustom_exceptions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2879\u001b[0m             \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'level' is an invalid keyword argument for this function"
     ]
    }
   ],
   "source": [
    "# coding=utf8\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from fkp_input import load_train_set, BatchRenderer\n",
    "from model1 import SingleHiddenNN\n",
    "\n",
    "# ======== 這是 tensorflow 吃 program argument 的方式 ========\n",
    "FLAGS = tf.app.flags.FLAGS  \n",
    "tf.app.flags.DEFINE_float('lr', 1e-2, 'learning rate')\n",
    "tf.app.flags.DEFINE_float('valid', 0.2, 'fraction of validation set')\n",
    "tf.app.flags.DEFINE_integer('n_epoch', 400, 'number of epochs')\n",
    "tf.app.flags.DEFINE_integer('batch_size', 128, 'batch size')\n",
    "tf.app.flags.DEFINE_integer('hidden', 100, 'batch size')\n",
    "\n",
    "# Global settings\n",
    "szImg = 96\n",
    "n_x = szImg * szImg\n",
    "n_y = 30\n",
    "\n",
    "\n",
    "def main(args=None):  # pylint: disable=unused-argument\n",
    "    nn = SingleHiddenNN(\n",
    "        input_shape=[FLAGS.batch_size, n_x],\n",
    "        n_hidden=FLAGS.hidden,\n",
    "        n_y=n_y)\n",
    "    datasets = load_train_set(valid=FLAGS.valid)\n",
    "    batches = BatchRenderer(\n",
    "        datasets.train.images, \n",
    "        datasets.train.labels,\n",
    "        FLAGS.batch_size)\n",
    "    nn.train(\n",
    "        batches, \n",
    "        datasets.valid, \n",
    "        lr=FLAGS.lr, \n",
    "        n_epoch=FLAGS.n_epoch)\n",
    "    display_img_and_prediction(datasets.valid, nn, 4, \n",
    "        'imgs/model1-(fully-connected)-face.png')\n",
    "\n",
    "\n",
    "def display_img_and_prediction(\n",
    "    valid_set,\n",
    "    nn,\n",
    "    n=4, \n",
    "    oPngName='imgs/model1-(fully-connected)-face.png'):\n",
    "    '''\n",
    "    Plot the results\n",
    "    '''\n",
    "    N = n * n\n",
    "    img = valid_set.images[:N]\n",
    "    img = np.reshape(img, (N, -1))\n",
    "    y = valid_set.labels[:N]\n",
    "    p = nn.predict(img)\n",
    "    \n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(N):\n",
    "        plt.subplot(n, n, i+1)\n",
    "        pn = p[i]\n",
    "        yn = y[i]\n",
    "        plt.imshow(img[i].reshape((96, 96)), cmap='gray')\n",
    "        plt.scatter(yn[0::2] * 48+48, yn[1::2] * 48+48,\n",
    "        # plt.scatter(yn[0::2] * 96, yn[1::2] * 96,\n",
    "            marker='o', edgecolors='r', facecolors='none')\n",
    "        plt.scatter(pn[0::2] * 48+48, pn[1::2] * 48+48,\n",
    "        # plt.scatter(pn[0::2] * 96, pn[1::2] * 96,\n",
    "            marker='x', edgecolors='b')\n",
    "        plt.axis('off')\n",
    "        plt.subplots_adjust(\n",
    "            left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "    \n",
    "    plt.savefig(oPngName)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()    # Run啥? 就是 run 本 script 中的 main() (TF根本莫名其妙)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 說明\n",
    "```python\n",
    "FLAGS = tf.app.flags.FLAGS  \n",
    "tf.app.flags.DEFINE_float()\n",
    "```\n",
    "這段是在指定 main script 的 input arguments\n",
    "\n",
    "然後我們先跳到最下面去看：  \n",
    "```python\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()\n",
    "```\n",
    "\n",
    "我為了配合 TF 吃 input argument 的寫法，所以用 `tf.app.run` 來觸發 `main`  \n",
    "這個寫法很讓人困惑：因為根本沒有哪個地方能看出來 `tf.app.run` 會觸發 `main` (但TF就是這樣 = = )  \n",
    "這也是我很不喜歡 TF 的地方 (sigh~)\n",
    "\n",
    "\n",
    "然後回到 `main`，這邊沒有什麼可講的，就是一般的步驟而已\n",
    "1. 讀 data\n",
    "2. 做成 bacthes (我是用 iterator 來做)\n",
    "3. 宣告 model，開始 training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Computational Graph\n",
    "這是 `tf.train.SummaryWriter` 的功用。\n",
    "當我們創造這個物件時，它會在 `logdir` 中建立一些資料，以便 Tensorboard 使用。  \n",
    "它可以存 image、audio、變數的 histogram，\n",
    "但我們這邊只用到了它可以 visualize 整張 computational graph 的功能。\n",
    "(所以後面都沒再繼續用它了)\n",
    "\n",
    "當你執行過主程式之後。就可以在用下列指令發動 Tensorboard  \n",
    "`tensorboard --logdir=[your_log_dir]`\n",
    "\n",
    "然後可以到 localhost:6006 去看結果(graph 就像前面一章的圖那樣)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小結\n",
    "我不確定我是否有遺漏什麼部分，但能做到這些，基本上已經可以算是入了 Tensorflow 的門了。\n",
    "資料夾裡應該還有 CNN 的範例 (model2)。  \n",
    "但我要再過一段時間才能寫講解了。  \n",
    "有問題再直接問我吧~"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
