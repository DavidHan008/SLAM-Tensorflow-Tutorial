{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Slim to define your models\n",
    "\n",
    "在開始之前，我先介紹一下 Slim。  \n",
    "他是 Tensorflow 眾多 wrapper 的一個。  \n",
    "在安裝 Tensorflow 的時候就已經內建。非常好用。  \n",
    "早期沒有 wrapper 的時候，我還得自己寫 Xavier initializer  \n",
    "還有每層的 w, b 也都要寫。相當繁複。\n",
    "有了 slim ，少下的功夫不知道有多少... QQ  \n",
    "另外，slim 回傳的東西都是 tf 物件。所以跟 Tensorflow 的 interoperability 非常高!\n",
    "\n",
    "Slim 一個強大的功能是：slim.arg_scope  \n",
    "他是用來預先填寫 arguments 用的，可以為你省下極多工夫。  \n",
    "我們直接看下列的例子就知道。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"fully_connected_11/Relu:0\", shape=(?, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.contrib import slim\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def dnn_transform(x, shapes):\n",
    "    with slim.arg_scope(\n",
    "        [slim.fully_connected],\n",
    "        activation_fn=tf.nn.relu):\n",
    "        for fan_out in shapes:\n",
    "            x = slim.fully_connected(x, fan_out)\n",
    "    return x\n",
    "\n",
    "\n",
    "x = tf.placeholder(dtype=tf.float32, shape=[None, 513])\n",
    "y = dnn_transform(x, [1024, 1024, 10])\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這樣你就有一個 feed-forward DNN 了。  \n",
    "他有兩層 1024-node 的 hidden，output dimension 為 10。  \n",
    "要多深有多深，反正你自己給定 shape 就是了。\n",
    "\n",
    "你可能會覺得奇怪：\n",
    "1. 為什麼一個 weight matrix 都沒看到? 那我以後要怎麼使用 model?  \n",
    "   A: 我們可以 load 整個 model。屆時，個別的 w 長什麼樣根本不重要。  \n",
    "2. 中間那些 hidden layer 怎麼都不見了?\n",
    "   A: 在大多數的情況下，hidden 是什麼根本不重要；  \n",
    "      你想要的話，把他們加入 return 吧。  \n",
    "      或者你也可以直接從 graph 攔截。  \n",
    "      (我沒打算寫這個部分，因為我從來沒用到過)\n",
    "\n",
    "你可以進一步透過 TensorBoard 來看你的架構。\n",
    "但 TensorBoard 的用法，容我之後再說。\n",
    "\n",
    "注意：\n",
    "1. 這邊把 dnn 寫成一個 function  \n",
    "   如果你呼叫他 N 次，你會得到 N 個「不同的」DNN  \n",
    "   如果這不是你要的，請使用 `tf.make_template`。(我們底下的例子會講)  \n",
    "1. 另一個要提醒的是：我們通常會把 network architecture 放在一個 json 檔裡。  \n",
    "   但以下為了方便都沒有這樣做。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Conv_8/Relu:0\", shape=(?, 8, 4, 256), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def cnn_encoder(x, shapes, is_training=False):\n",
    "    with slim.arg_scope(\n",
    "        [slim.batch_norm],\n",
    "        scale=True,\n",
    "        updates_collections=None,\n",
    "        is_training=is_training,\n",
    "        scope='BN'):\n",
    "        with slim.arg_scope(\n",
    "            [slim.conv2d],\n",
    "            kernel_size=[3, 3],\n",
    "            stride=[2, 2],\n",
    "            normalizer_fn=slim.batch_norm,  # BN here\n",
    "            activation_fn=tf.nn.relu):\n",
    "            for fan_out in shapes:\n",
    "                x = slim.conv2d(x, fan_out)\n",
    "    return x\n",
    "\n",
    "\n",
    "x = tf.placeholder(dtype=tf.float32, shape=[None, 64, 32, 3])\n",
    "y = cnn_encoder(x, [64, 128, 256])\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "就這樣，其實和定義 fully-connected layer 幾乎一樣。  \n",
    "\n",
    "注意  \n",
    "1. 圖片預設的維度是 [batch size, height, width, color channel]  \n",
    "   而 kernel size 和 stride 都是對 [height, width] 去定義的。  \n",
    "2. stride 就是 down-/up-sampling rate。\n",
    "   當你使用 conv2d 的時候，stride 對應的就是 down-sampling rate  \n",
    "   當你使用 conv2d_transpose 的時候，stride 對應的就是 up-sampling rate  \n",
    "3. kernel size 就是 num of filter taps\n",
    "\n",
    "### Batch Normalization (BN)\n",
    "在這個例子中，我另外也展示了 batch normalization (BN) layer 的用法。  \n",
    "由於 SLIM 都幫我們包好了，我們只要在 conv2d 裡面啟用 batch_norm 即可。  \n",
    "需注意的是：凡使用 BN 必定要指定是否是 training (因為 BN 在 training 和 testing 的行為不同)。  \n",
    "因此一定要注意 is_training 這個 argument 有沒有設對。  \n",
    "注意：  \n",
    "1. operation 的順序：\n",
    "  1. convolution\n",
    "  2. bias or BN\n",
    "  3. activation\n",
    "2. updates_collections = None 是最簡便的方式。\n",
    "   最好的方式是要去設定 control flow，運算會比較快，但我不會弄 QQ\n",
    "3. 當你用 normalizer_fn 的時候，bias 會自動被 disable 掉 (因為 BN 也有 bias term)。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN\n",
    "OK，到這邊為止，創建 model 的部分就告終了。  \n",
    "現在你已經有能力可以創建一個 DNN、CNN。  \n",
    "那我們就來測試一下怎麼實作一個 GAN model 吧\n",
    "\n",
    "首先，GAN有什麼部件?  \n",
    "z --G--> xh --D--> T/F\n",
    "只有一個 generator 和一個 discriminator。  \n",
    "最多就是還要一個 sampler 讓你可以產生 random sample  \n",
    "因此API就是  \n",
    "1. generate: xh = G(z)  \n",
    "2. discriminate: likelihood ratio = D(x)  \n",
    "3. sample  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GAN(object):\n",
    "    def __init__(self, arch, is_training):\n",
    "        pass\n",
    "    def generate(self, z):\n",
    "        pass\n",
    "    def discriminate(self, x):\n",
    "        pass\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To Be Continued after I'm back from Jeju**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
